{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook we can see the application of several classification models to predict whether a team will make the playoffs in the following year. "
   ],
   "id": "3ba795f30b52c548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It takes data from years 1 to 9 for training the models, and data from year 10 for testing. Here's an overview of how the script works:\n",
    "\n",
    "First, the dataset is loaded, and the data is split into training and testing sets. The training set consists of data from years 1 through 9, while the testing set consists of data from year 10. \n",
    "\n",
    "Then we selected the features (i.e., the input variables) by excluding the target variable (`PlayOffNextYear`), as well as team-specific identifiers like `tmID` and `year`. To ensure the data is properly scaled, the features are standardized using `StandardScaler`.\n",
    "\n",
    "Then we define a range of classification models to evaluate, including Logistic Regression, K-Nearest Neighbors (KNN), Random Forest, Naive Bayes, Decision Tree, XGBoost, Gradient Boosting, and Support Vector Machine (SVM). For the XGBoost and Gradient Boosting models, we used SMOTE (Synthetic Minority Over-sampling Technique). \n",
    "\n",
    "Once the models are defined and the data is prepared, the following step was to train each model using the scaled training data. For the models that use SMOTE, the training was done on the balanced dataset. \n",
    "\n",
    "After training, we were able to guarantee to the models to make predictions and calculate several evaluation metrics for each model, including accuracy, precision, recall, and F1 score. These metrics help determine how well the models are performing.\n",
    "\n",
    "Additionally, we used `ClassificationUtils` to generate visualizations such as confusion matrices and learning curves, which help in understanding how each model behaves and where it might be making mistakes. In this way we had a deeper insight into the models' performance, and it helped us to identify areas for improvement.\n",
    "\n",
    "Finally, we collected all the results in a summary table, showing the performance of each model across all the metrics. In this way we made comparison between the models to see the best possible outcome."
   ],
   "id": "47997a76dac08b1c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from classification_utils import ClassificationUtils\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "file_path = '../newData/Shifted_playoff.csv' \n",
    "df = pd.read_csv(file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "train_df = df[df['year'] < 10]\n",
    "test_df = df[df['year'] == 11]\n",
    "\n",
    "X_train = train_df.drop(columns=[\"PlayOffNextYear\", \"tmID\", \"year\"])\n",
    "y_train = train_df[\"PlayOffNextYear\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"PlayOffNextYear\", \"tmID\", \"year\"])\n",
    "y_test = test_df[\"PlayOffNextYear\"] \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "id": "6857503ee42b001b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(max_depth=5, min_samples_leaf=10, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, min_samples_leaf=10, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(max_depth=2, n_estimators=50, learning_rate=0.05, reg_lambda=15, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(max_depth=2, n_estimators=50, learning_rate=0.05, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"linear\", probability=True)\n",
    "}\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    if model_name in [\"XGBoost\", \"Gradient Boosting\"]:\n",
    "       \n",
    "        print(f\"Training {model_name} with SMOTE-balanced dataset...\")\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        predictions = model.predict(X_train_balanced)\n",
    "        accuracy = accuracy_score(y_train_balanced, predictions)\n",
    "        precision = precision_score(y_train_balanced, predictions)\n",
    "        recall = recall_score(y_train_balanced, predictions)\n",
    "        f1 = f1_score(y_train_balanced, predictions)\n",
    "\n",
    "        results[f\"{model_name} (SMOTE)\"] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1\n",
    "        }\n",
    "\n",
    "        ClassificationUtils.plot_confusion_matrix(\n",
    "            y_train_balanced, predictions, model_name=f\"{model_name} (SMOTE)\"\n",
    "        )\n",
    "        ClassificationUtils.plot_learning_curve(\n",
    "            estimator=model,\n",
    "            X=X_train_balanced,\n",
    "            y=y_train_balanced,\n",
    "            title=f\"Learning Curve: {model_name} (SMOTE)\",\n",
    "            cv=5\n",
    "        )\n",
    "    else:\n",
    "       \n",
    "        print(f\"Training {model_name}...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        predictions = model.predict(X_train_scaled)\n",
    "\n",
    "        accuracy = accuracy_score(y_train, predictions)\n",
    "        precision = precision_score(y_train, predictions)\n",
    "        recall = recall_score(y_train, predictions)\n",
    "        f1 = f1_score(y_train, predictions)\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1\n",
    "        }\n",
    "\n",
    "        ClassificationUtils.plot_confusion_matrix(\n",
    "            y_train, predictions, model_name=model_name\n",
    "        )\n",
    "        ClassificationUtils.plot_learning_curve(\n",
    "            estimator=model,\n",
    "            X=X_train_scaled,\n",
    "            y=y_train,\n",
    "            title=f\"Learning Curve: {model_name}\",\n",
    "            cv=5\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Model evaluation results:\")\n",
    "display(results_df)"
   ],
   "id": "bce2e69980b09227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cross-Validation Results\n",
    "In this section we applied cross-validation to the XGBoost and Gradient Boosting models to evaluate their performance. First, we defined a `StratifiedKFold` object for 5 splits and sets up a loop to apply this cross-validation technique to both the XGBoost and Gradient Boosting models. \n",
    "For each model, we performed the cross-validation and calculated the mean accuracy and standard deviation of the accuracy scores across the 5 folds. These results are stored in a dictionary and displayed in a summary table.\n",
    "\n",
    "Afterward, we ran a standard 5-fold cross-validation for both boosting models (without the stratified approach) and calculates similar metrics: the mean accuracy and standard deviation. The results are stored in another dictionary and displayed in a separate table.\n"
   ],
   "id": "aa64063b3c1d1e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv_results = {}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name in [\"XGBoost\", \"Gradient Boosting\"]:\n",
    "    model = models[model_name]\n",
    "    print(f\"Performing stratified cross-validation for {model_name}...\")\n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        \"Mean Accuracy\": scores.mean(),\n",
    "        \"Std Dev\": scores.std()\n",
    "    }\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results).T\n",
    "print(\"Stratified Cross-Validation results for boosting models:\")\n",
    "display(cv_results_df)\n",
    "\n",
    "cv_results_boosting = {}\n",
    "\n",
    "for model_name in [\"XGBoost\", \"Gradient Boosting\"]:\n",
    "    model = models[model_name]\n",
    "    print(f\"Performing cross-validation for {model_name}...\")\n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    cv_results_boosting[model_name] = {\n",
    "        \"Mean Accuracy\": scores.mean(),\n",
    "        \"Std Dev\": scores.std()\n",
    "    }\n",
    "\n",
    "cv_results_boosting_df = pd.DataFrame(cv_results_boosting).T\n",
    "print(\"Cross-validation results for boosting models:\")\n",
    "display(cv_results_boosting_df)\n"
   ],
   "id": "a585dff00efba624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test Set Evaluation\n",
    "Finally, we evaluated each model on the test set (Year 10) to assess their generalization performance. \n",
    "First therei s the training of the model, then probabilities' prediction and normalization.\n"
   ],
   "id": "65e103dfb4661425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Making predictions for {model_name} on Year 10...\")\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    probabilities = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    normalized_probabilities = 8 * probabilities / probabilities.sum()\n",
    "    test_df[f\"{model_name}_Predicted_Probability\"] = probabilities\n",
    "    test_df[f\"{model_name}_Normalized_Probability\"] = normalized_probabilities\n",
    "\n",
    "display(test_df)\n"
   ],
   "id": "8fabb79276bc2ae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prediction for Year 10 with Playoff Constraints\n",
    "\n",
    "In this phase, we used the best model to:\n",
    "\n",
    "1. Predict the probabilities of playoff qualification for the teams in Year 10.\n",
    "2. Normalize the probabilities to ensure the sum equals 8.\n",
    "3. Select the top 4 teams from each conference.\n",
    "4. Add a label identifying the teams selected for the playoffs."
   ],
   "id": "3aa9d411ad15f699"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = RandomForestClassifier(max_depth=5, min_samples_leaf=10, random_state=42)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "probabilities = best_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1 (playoff)\n",
    "\n",
    "test_df = test_df.copy()  # Explicit copy\n",
    "test_df.loc[:, \"Predicted_Probability\"] = probabilities\n",
    "\n",
    "normalized_probabilities = 8 * probabilities / probabilities.sum()\n",
    "test_df.loc[:, \"Normalized_Probability\"] = normalized_probabilities\n",
    "\n",
    "east_teams = test_df[test_df[\"confID\"] == 1].nlargest(4, \"Normalized_Probability\")\n",
    "west_teams = test_df[test_df[\"confID\"] == 0].nlargest(4, \"Normalized_Probability\")\n",
    "\n",
    "playoff_teams = pd.concat([east_teams, west_teams])\n",
    "\n",
    "test_df.loc[:, \"Selected_Playoff\"] = test_df[\"tmID\"].isin(playoff_teams[\"tmID\"]).astype(int)\n",
    "\n",
    "print(\"Selected playoff teams:\")\n",
    "display(playoff_teams[[\"tmID\", \"confID\", \"Normalized_Probability\"]])\n",
    "\n",
    "print(\"Complete table with probabilities and selections:\")\n",
    "display(test_df[[\"tmID\", \"confID\", \"Predicted_Probability\", \"Normalized_Probability\", \"Selected_Playoff\"]])\n"
   ],
   "id": "787fe37af0319ebc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
